---
title: "Tests of the General Linear Hypothesis in R"
author: "Wade K. Copeland"
date: last-modified
date-format: "MMMM DD, YYYY"
bibliography: references.bib
output-file: index.html
format:
    html:
        embed-resources: true
        code-overflow: wrap
        page-layout: full
        code-fold: false
        toc: true
        toc-location: left
        toc-depth: 4
        toc-expand: 4
        html-math-method: mathjax
        code-tools:
            source: true
            toggle: true
            caption: "Source Code"
number-sections: false
appendix-style: plain
engine: jupyter
knitr:
    opts_knit:
        root.dir: "./"
---

# Introduction

When parameters of a general linear model are estimated, analysts often report the main effects.  However, sometimes the hypothesis of interest is a linear combination of the main effects that is not displayed by default in a standard regression table.  Testing many hypotheses from the same linear model is especially relevant when an analyst fits a model using categorical variables where many post-hoc hypotheses are of interest.  This presentation will show how to code a categorical variable for use in the general linear model and use tests of the so-called general linear hypothesis to test any number of hypotheses about the effects [@fox2008applied].

This tutorial uses the R programming language [@R2020].  All of the files needed to reproduce these results can be downloaded from the Git repository <a href="https://github.com/wkingc/general-linear-hypothesis" target="_blank">https://github.com/wkingc/general-linear-hypothesis</a>.

# Required Libraries

The libraries <i>knitr</i>, <i>bookdown</i>, and <i>kableExtra</i> are loaded to generate the HTML output [@kintr2021; @bookdown2020; @kableExtra2020].  The library <i>magrittr</i> is loaded so we can use the pipe (%>%) operator [@magrittr2020].  The library <i>DT</i> is loaded to display simulated data in an output table [@dt2021].  Data summaries are facilitated by the <i>dplyr</i> library [@dplyr2021].  The library <i>multcomp</i> is loaded so we can do tests of the general linear hypothesis [@multcomp2008; @fox2008applied].

```{r libraries, eval = TRUE, echo = TRUE, results = 'hide', warning = FALSE, message = FALSE}
library("knitr")
library("bookdown")
library("kableExtra")
library("magrittr")
library("DT")
library("dplyr")
library("multcomp")
```

# Data Generating Process

Suppose we have a set of $i \in \{1, ... , n\}$ independent and identically distributed normal random variables each denoted by $y_i$, whose value depends on a categorical variable with three levels, $x \in \{l, m, h\}$ that correspond to <i>low</i>, <i>medium</i>, and <i>high</i>, respectively.  We will further assume that $E[y_i|x] = \mu_x$ with constant variance $\sigma^2$.  The data generating process is written concisely in @eq-dgp.

$$
\begin{equation}
\begin{split}
y_i|x \sim N(\mu_{x}, \sigma^2)
\end{split}
\end{equation}
$$ {#eq-dgp}

# Simulation Data

Simulating data using this data generating process is straight forward.  We will assume $\mu_{x = l} = 1$, $\mu_{x = m} = 1$, $\mu_{x = h} = 2$ with fixed variance $\sigma^2 = 1$ and generate a single realization for each of $n = 100$ normal random variables.

```{r simulatedTab, eval = TRUE, echo = TRUE, results = TRUE, warning = FALSE, message = FALSE}
set.seed(123)

# Create an empty data frame to hold the simulated data.
d <- matrix(nrow = 100, ncol = 2)
colnames(d) <- c("Y", "X")
d <- as.data.frame(d)

# We assume without loss of generality that each of the categories is equally likely to be observed.
X <- sample(x = c("l", "m", "h"), size = 100, replace = TRUE, prob = c(1/3, 1/3, 1/3))
d$X <- X

# Since the distribution of each y_i depends on the value of x, we generate the outcome separately for each possible value.
d[which(d$X == "l"), "Y"] <- rnorm(n = length(which(d$X == "l")), mean = 1, sd = 1)
d[which(d$X == "m"), "Y"] <- rnorm(n = length(which(d$X == "m")), mean = 1, sd = 1)
d[which(d$X == "h"), "Y"] <- rnorm(n = length(which(d$X == "h")), mean = 2, sd = 1)

# The factor function tells R that X is a factor and should be treated as such in downstream analyses.
d$X <- factor(d$X, levels = c("l", "m", "h"))

# We can use the datatable function to display the simulated data.
datatable(
    d, escape = FALSE, 
    caption = "This table contains the simulated data from the data generating process.  Each value in the Y column is a realization of an independent and identically distributed normal random variable whose expectation is conditional on the value in the X column.",
    extensions = c('Buttons', 'KeyTable'),
    class = 'cell-border stripe',
    rownames = TRUE,
    options = list(
        dom = 'Bfrtip',
        pageLength = 10,
        deferRender = TRUE,
        responsive = TRUE,
        scrollX = TRUE,
        scrollCollaspe = TRUE,
        paging = TRUE,
        autoWidth = FALSE,
        keys = TRUE,
        buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
    ))
```

@tbl-simulated-tab-summaries shows the estimated mean and variance for the outcome for each level of the categorical variable in the simulated data set.

```{r simulatedTabSummaries, eval = TRUE, echo = TRUE, results = TRUE, warning = FALSE, message = FALSE}
#| label: tbl-simulated-tab-summaries
#| tbl-cap: "The estimated mean and variance of the outcome for each value of the categorical variable in the simulated data."

dSumarries <- d %>% 
    group_by(X) %>%
    summarize(
        mean = round(mean(Y, na.rm = TRUE), 2),
        variance = round(sd(Y, na.rm = TRUE)^2, 2)
    )

dSumarries %>%
    kbl(escape = FALSE, table.attr = "data-quarto-disable-processing=true") %>%
    kable_classic_2(full_width = FALSE) %>% 
    column_spec(column = 1, width = "1em") %>% 
    column_spec(column = 2, width = "5em") %>% 
    column_spec(column = 3, width = "5em")
```

# Dummy Coding the Categorical Predictor

We can write $E[y|x] = \mu_x$ as a linear combination of a constant and $x$ by dummy coding $x$.  Dummy coding transforms $x$ into a set of dichotomous predictors for all but one of the levels which we call the reference category.  In the current example, dummy coding says that $x_{l} = 1$ if in the <i>low</i> category and $x_{l} = 0$ otherwise; $x_{m} = 1$ if in the <i>medium</i> category and $x_{m} = 0$ otherwise; $x_{h} = 1$ if in the <i>high</i> category and $x_{h} = 0$ otherwise.

If we treat $x_l$ as the referent category, the expectation of $y$ can be written as a linear function of the categorical predictor (@eq-linear-predictor).

$$
\begin{equation}
\begin{split}
E[y|x] = \alpha + \beta_1 x_{m} + \beta_2 x_{h}
\end{split}
\end{equation}
$$ {#eq-linear-predictor}

For the simulated data, the design matrix for the dummy coded categorical variable is shown below.

```{r designMatrixTab, eval = TRUE, echo = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# The model.matrix function extracts the design matrix for a given formula object.
dDM <- model.matrix(as.formula(Y ~ X), data = d)

# We can use the datatable function to display the design matrix for the simulated data.
datatable(
    dDM, escape = FALSE, 
    caption = "The design matrix for the simulated data that shows the dummy coding for the categorical variable.",
    extensions = c('Buttons', 'KeyTable'),
    class = 'cell-border stripe',
    rownames = TRUE,
    options = list(
        dom = 'Bfrtip',
        pageLength = 10,
        deferRender = TRUE,
        responsive = TRUE,
        scrollX = TRUE,
        scrollCollaspe = TRUE,
        paging = TRUE,
        autoWidth = FALSE,
        keys = TRUE,
        buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
    ))
```

# A Simple Example

Under the dummy coding scheme, the mean of $y$ for each level of the categorical variable can be derived from the linear predictor.

* The mean value of $y$ when the categorical variable is <i>low</i> is $\mu_{x=l} = E[y|x_m = 0, x_h = 0] = \alpha$.
* The mean value of $y$ when the categorical variable is <i>medium</i> is $\mu_{x=m} = E[y|x_m = 1, x_h = 0] = \alpha + \beta_1$.
* The mean value of $y$ when the categorical variable is <i>high</i> is $\mu_{x=h} = E[y|x_m = 0, x_h = 0] = \alpha + \beta_2$.

The first step in deriving these effects for the current data is to fit a linear model to the data and extract the parameter estimates for the main effects.

```{r modelParameters, eval = TRUE, echo = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Estimate the parameters for the linear model
fit <- lm(Y~X, data = d)
fit
```

Next, we create a matrix containing a single row for each linear effect we care about.  The linear effect matrix is multiplied by the vector of estimated coefficients for the main effects to get the effects of interest.

```{r mValues, eval = TRUE, echo = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Matrix that represents each linear effect we want to estimate.
mMat <- rbind(c(1, 0, 0), c(1, 1, 0), c(1, 0, 1))
mMat

# Vector of estiamted coefficients from the linear model
cMat <- matrix(coef(fit), nrow = 3)
cMat

# The estimated value of y for each value of the category.
mMat %*% cMat
```

To test the null hypothesis that any of these estimated effects are zero, we can apply the general linear hypothesis [@fox2008applied].

* $H_0: \alpha = 0$ tests the null hypothesis that the mean value of $y$ is zero when the categorical variable is <i>low</i> ($x = l$).
* $H_0: \alpha + \beta_1 = 0$ tests the null hypothesis that the mean value of $y$ is zero when the categorical variable is <i>medium</i> ($x = m$).
* $H_0: \alpha + \beta_2 = 0$ tests the null hypothesis that the mean value of $y$ is zero when the categorical variable is <i>high</i> ($x = h$).

```{r mainEffectsTest, eval = TRUE, echo = TRUE, results = TRUE, warning = FALSE, message = FALSE}
mTest <- glht(model = fit, linfct = mMat)
summary(mTest)
```

# A More Complicated Example

Specifying linear hypotheses allows us to test any number of effects inferentially.

* $H_0: \mu_{x=l} - \mu_{x = h} = \alpha - (\alpha + \beta_2) = -\beta_2 = 0$ tests the null hypothesis that the mean value of $y$ in the referent category <i>low</i> ($x = l$) is the same as the mean value of $y$ in the <i>high</i> category ($x = h$).

We can get even more fancy and derive the mean value of $y$ that is marginalized over the <i>medium</i> and <i>high</i> categories.

* $H_0: \mu_{x=l} - \frac{(\mu_{x=m} + \mu_{x = h})}{2} = \alpha - \frac{\alpha + \beta_1 + \alpha + \beta_2}{2} = - \frac{1}{2}\beta_1 - \frac{1}{2}\beta_2 = 0$ tests the null hypothesis that the mean value of $y$ in the referent category <i>low</i> ($x = l$) is the same as the mean value of $y$ with in either the <i>medium</i> or <i>high</i> categories ($x = m$ or $x =l$).

```{r otherEffectsTest, eval = TRUE, echo = TRUE, results = TRUE, warning = FALSE, message = FALSE}
oMat <- rbind(c(0, 0, -1), c(0, -1/2, -1/2))
oMat

oTest <- glht(model = fit, linfct = oMat)
summary(oTest)
```

# Multiple Comparisons

Unless the goal is to publish in the Journal of Irreproducible Results, an analyst should adjust for simultaneously testing multiple hypotheses.

The multiple testing problem occurs because our fixed and known probability of falsely rejecting a true null hypothesis (usually $5\%$) applies only to a single test and increases as a function of the total number of inferences. For example, suppose we do 20 simultaneous tests, each with a $5\%$ probability of being a false discovery. In that case, we expect at least one of these inferences will result in falsely rejecting a true null hypothesis.

There are two commonly used methods for adjusting for multiple comparisons.  The first controls for the probability of falsely rejecting at least one true null hypothesis among all hypotheses tested, also known as the family-wise error rate (FWER), and typified by the Bonferroni correction [@dunn1961multiple].  The second controls the probability of falsely rejecting at least one true null hypothesis among rejected hypotheses, known as the false discovery rate (FDR) [@benjamini1995controlling].

@tbl-res-tab shows the inferential results after adjusting for both the FWER and FDR.

```{r resTab, eval = TRUE, echo = TRUE, results = TRUE, warning = FALSE, message = FALSE}
#| label: tbl-res-tab
#| tbl-cap: "The results for all analyses with inferential results controlling the family-wise error rate (FWER) and false discovery rate (FDR)."
resTab <- rbind(
    cbind(summary(mTest)$test$coefficients, summary(mTest)$test$sigma, summary(mTest)$test$tstat, summary(mTest)$test$pvalues),
    cbind(summary(oTest)$test$coefficients, summary(oTest)$test$sigma, summary(oTest)$test$tstat, summary(oTest)$test$pvalues)
)

rownames(resTab) <- c("\\(H_0: \\mu_{x = l} = 0\\)", "\\(H_0: \\mu_{x = m} = 0\\)", "\\(H_0: \\mu_{x = h} = 0\\)", "\\(H_0: \\mu_{x = l} - \\mu_{x = h} = 0\\)", "\\(H_0: \\mu_{x = l} - \\frac{1}{2}(\\mu_{x = m} + \\mu_{x = h}) = 0\\)")

resTab <- cbind(resTab, p.adjust(resTab[, 4], method = "bonferroni"), p.adjust(resTab[, 4], method = "fdr"))

colnames(resTab) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)", "FWER Pr(>|t|)", "FDR Pr(>|t|)")

resTab %>%
    kbl(format = 'html', escape = FALSE, table.attr = "data-quarto-disable-processing=true") %>%
    kable_classic_2()
```

# R Session Information

```{r sessionInfo, eval = TRUE, echo = TRUE, results = TRUE, warning = FALSE, message = FALSE}
sessionInfo()
```

# References
